{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366edae6-c36d-4b71-9d27-83de8f4d7ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Nichtlineare Optimierung mit Nebenbedingungen\n",
    "\n",
    "## Dr. Anselm Hudde - 24.01.2022\n",
    "\n",
    "![](optimization.png)\n",
    "**Quelle: Venter, G. (2010). Review of optimization techniques.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda742a-3bc3-4d75-9c5d-867099ff875c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run nonlinear_programming.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e78631",
   "metadata": {},
   "source": [
    "## Worum geht es bei der nichtlinearen Optimierung?\n",
    "\n",
    "Bei der Optimierung geht es darum, das Minimum (bzw. das Maximum) einer Funktion $f$ zu finden.\n",
    "\n",
    "- Wie maximiert ein Unternehmen den Ertrag, unter Ber√ºcksichtigung der vorhandenen Ressourcen?\n",
    "\n",
    "- Wie erreicht man mit einer Portfolioallokation eine maximale Rendite, wenn die erlaubte Volatilit√§t beschr√§nkt ist?\n",
    "\n",
    "- Wie kann man die richtigen Modellparameter sch√§tzen (statistische Modelle, neuronale Netze)?\n",
    "\n",
    "## Beispiel aus der Portfoliooptimierung:\n",
    "\n",
    "Ein Investor will $10\\,000$ Euro investieren, wobei diese auf zwei Aktien, $A$ und $B$, aufgeteilt werden.\n",
    "\n",
    "- Erwartete j√§hrliche Renditen: \n",
    "\n",
    "    Aktie $A$: $r_A = 7\\%$\n",
    "\n",
    "    Aktie $B$: $r_{A} = 9\\%$\n",
    "    \n",
    "    \n",
    "- Volatilit√§ten:\n",
    "\n",
    "    Aktie $A$: $\\sigma_A = 20\\%$\n",
    "\n",
    "    Aktie $B$: $\\sigma_{B} = 30\\%$\n",
    "    \n",
    "    \n",
    "- Korrelation:\n",
    "\n",
    "    $\\rho_{A, B} = 0.7$\n",
    "\n",
    "Der Investor m√∂chte sein Geld so anlegen, dass er eine erwartete Rendite von $5\\%$ erreicht, und dabei so wenig Volatilit√§t wie m√∂glich zulassen.\n",
    "Die Volatilit√§t eines Portfolios bestehend aus $x_1 \\times 10\\,000$ ‚Ç¨ in Aktie $A$ und $x_2 \\times 10\\,000$ ‚Ç¨ in Aktie $B$ berechnet sich als\n",
    "\n",
    "\\begin{equation}\n",
    "f (x_{1}, x_{2})\n",
    "=\n",
    "\\sqrt{ \\sigma_{A}^2 x_1^2 + \\sigma_{B}^2 x_2^2 + 2 \\sigma_{A} x_1 \\sigma_{B} x_2 \\rho_{A, B}}.\n",
    "\\end{equation}\n",
    "\n",
    "Unser Ziel ist also: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{Minimiere }& f(x) & \\textit{ (die Volatilit√§t)}\\\\\n",
    "\\text{wobei }& x_1 + x_2 - 1 \\leq 0 & \\textit{ (nicht mehr als 100% investieren)}\\\\\n",
    "\\text{und }& r_{A} x_{1} + r_{B} x_{2} - 0.05 = 0 ~& \\textit{ (5% Rendite)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "gilt.\n",
    "Wenn wir\n",
    "\n",
    "\\begin{equation}\n",
    "g(x_1, x_2) = x_1 + x_2 - 1\n",
    "\\end{equation}\n",
    "und\n",
    "\\begin{equation}\n",
    "h(x_1, x_2) = r_{A} x_{1} + r_{B} x_{2} - 0.05\n",
    "\\end{equation}\n",
    "\n",
    "definieren, k√∂nnen wir unser Problem als Beispiel einer allgemeinen Formulierung eines Optimierungsproblems auffassen.\n",
    "\n",
    "## Formulierung eines nichtlinearen Optimierungsproblems mit Nebenbedingungen\n",
    "\n",
    "Sei $D \\subseteq \\mathbb{R}^n$ eine Teilmenge, und seien\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f \\colon D \\to \\mathbb{R} \\\\\n",
    "g \\colon D \\to \\mathbb{R} \\\\\n",
    "h \\colon D \\to \\mathbb{R}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "drei (m√∂glicherweise nichtlineare) Funktionen.\n",
    "\n",
    "**Ziel der nichtlinearen Optimierung:**\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{Minimiere }& f(x) \\\\\n",
    "\\text{wobei }& g(x) \\leq 0 \\\\\n",
    "\\text{und }& h(x) = 0 \\text{ gilt.}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb3a6e-f567-4a67-8a00-e22646a019bb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## Beispielfunktion\n",
    "\n",
    "Wir betrachten die Funktion\n",
    "\n",
    "\\begin{equation}f(x, y) = \\sin(x) + 0.05 x^2 + 0.1 y^2, \\end{equation}\n",
    "\n",
    "die wir minimieren wollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85e606",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,f)\n",
    "surface_plot.show()\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406e17b-70d2-4f3f-81b2-3354f42acf0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Das Gradientenverfahren: Idee\n",
    "\n",
    "Die Idee ist es, immer in die Richtung zu gehen, in die es am steilsten bergauf- bzw. bergab geht.\n",
    "\n",
    "![](optimization.png)\n",
    "\n",
    "Aber wie finden wir diese Richtung? Daf√ºr m√ºssen wir die Steigung messen k√∂nnen. \n",
    "\n",
    "Doch wie messen wir die Steigung?\n",
    "Hier kommt der **Gradient** ins Spiel:\n",
    "\n",
    "**Definition:**\n",
    "Der Gradient der Funktion $f \\colon \\mathbb{R}^{n} \\to \\mathbb{R}$ ist die Funktion\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f \\colon \\mathbb{R}^{n} \\to \\mathbb{R}^{n};~~\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_{1} \\\\ \\vdots \\\\ x_{n}\n",
    "\\end{pmatrix}\n",
    "\\mapsto\n",
    "\\begin{pmatrix}\n",
    "\\tfrac{\\partial f}{\\partial x_{1}} (x) \\\\ \\vdots \\\\ \\tfrac{\\partial f}{\\partial x_{n}} (x)\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf3abe-4911-45c0-99b7-78eb125d5cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Anschauung:** Der Gradient zeigt immer in Richtung des st√§rksten Anstiegs.\n",
    "\n",
    "Beispiel 1: $f(x, y) = x$, $\\nabla f(x, y) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392913a-0e0c-4456-8938-893f47d3cd1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(0,4,0,4,function = lambda x: x[0])\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13632e34-1665-4598-8873-2bc804728365",
   "metadata": {},
   "source": [
    "Beispiel 2: $f(x, y) = \\tfrac{1}{2} (x+y)$, $\\nabla f(x, y) = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cac26d-b0e0-45c8-aa7e-34766f499044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(0,4,0,4,function = lambda x: (1/3)*(x[0]+x[1]))\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635e17b-370a-41e8-9254-f9d3139664f6",
   "metadata": {},
   "source": [
    "## Zur√ºck zur eigentlichen Funktion:\n",
    "\n",
    "Der Gradient von\n",
    "$ùëì(ùë•,ùë¶)= \\sin(ùë•)+0.05 ùë•^{2} + 0.1 ùë¶^{2}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(x, y)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\cos(x) + 0.1x \\\\ 0.2 y\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Beispiele: \n",
    "\\begin{equation}\n",
    "\\nabla f(-4, -2) =\n",
    "\\begin{pmatrix}\n",
    "-1.05 \\\\ -0.40\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\nabla f(-1, 2) =\n",
    "\\begin{pmatrix}\n",
    "0.44 \\\\ 0.20\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db4c94-24fe-4804-90df-351454a787df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return np.array([np.cos(x[0])+0.1*x[0], 0.2*x[1]])\n",
    "\n",
    "contour_plot.add_gradients(grad_f)\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5117f6f",
   "metadata": {},
   "source": [
    "## Die Schritte beim Gradientenverfahren\n",
    "\n",
    "Wir starten mit einem Punkt $x_0 \\in \\mathbb{R}^{2}$.\n",
    "Dann gehen wir im $n$-ten Schritt in die Gegenrichtung des Gradienten\n",
    "$\\nabla f(x, y)$, und zwar multipliziert mit der Lernrate $\\gamma_n$:\n",
    "\n",
    "\\begin{equation}\n",
    "\tx_{n+1}= x_{n} - \\gamma_{n} \\nabla f({x} _{n}),\\ n\\geq 0.\n",
    "\\end{equation}\n",
    "\n",
    "### Lernrate $\\gamma = 1$:\n",
    "\n",
    "Wir fangen im Punkt\n",
    "$x_0 = \\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}$\n",
    "an, und verwenden eine Lernrate von $\\gamma = 1$.\n",
    "Dann gilt\n",
    "\n",
    "\\begin{equation} \\nabla f(x_0) =\n",
    "\\begin{pmatrix}\\cos(-4)+0.1\\cdot(-4) \\\\ 0.2\\cdot(-2) \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} -1.05 \\\\ -0.40 \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "und damit\n",
    "\n",
    "\\begin{equation}\n",
    "x_{1} = x_{0} + \\gamma_{0} \\cdot \\nabla f(x_0)\n",
    "=\n",
    "\\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}\n",
    "- 1 \\cdot \n",
    "\\begin{pmatrix} -1.05 \\\\ -0.40 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "-2.95 \\\\ -1.60\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "Die n√§chsten Schritte folgen analog:\n",
    "\n",
    "$x_{2} = \\begin{pmatrix} -1.67 \\\\ -1.28\\end{pmatrix}, ~\n",
    "x_{3} = \\begin{pmatrix} -1.40 \\\\ -1.02\\end{pmatrix}$\n",
    "\n",
    "usw..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea11cba-4570-4aaf-b9cf-6e1f4b898999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Das Gradientenverfahren mit einer Lernrate von gamma = 1:\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return np.array([np.cos(x[0])+0.1*x[0], 0.2*x[1]])\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ea80e",
   "metadata": {},
   "source": [
    "In der folgenden Zelle werden die einzelnen Schritte des Gradientenverfahrens mit der Lernrate $\\gamma = 1$ ausgef√ºhrt.\n",
    "Wenn man diese Zelle mehrfach laufen l√§sst, sieht man, wie das Verfahren konvergiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5617af",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=1, Iterationen=i, color = \"#636EFA\")\n",
    "contour_plot.show()\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d308b5e-5660-44a2-aabe-cf2e385b9d8a",
   "metadata": {},
   "source": [
    "### Das Gradientenverfahren mit einer Lernrate von $\\gamma = 0.1$ ist langsamer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e363b-6d69-4bd9-84a6-9ca6a5d5fed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16681b",
   "metadata": {},
   "source": [
    "In der folgenden Zelle werden die einzelnen Schritte des Gradientenverfahrens mit der Lernrate $\\gamma = 0.1$ ausgef√ºhrt.\n",
    "Beim mehrfachen Ausf√ºhren der Zelle sieht man, dass das Verfahren sehr langsam konvergiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669954b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Gradientenverfahren mit einer Lernrate von gamma = 0.1:\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=0.1, Iterationen=i, color = \"#EF553B\")\n",
    "contour_plot.show()\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba80109",
   "metadata": {},
   "source": [
    "### Eine Lernrate von $\\gamma = 2$ f√ºhrt hingegen nicht zu einer Konvergenz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd4ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58aa31",
   "metadata": {},
   "source": [
    "In der folgenden Zelle werden die einzelnen schritte der Gradientenverfahrens mit der Lernrate $\\gamma = 2$ ausgef√ºhrt. \n",
    "Beim mehrfachen Ausf√ºhren der Zelle sieht man, dass das Verfahren nicht konvergiert, sondern die L√∂sung immer hin- und her springt.\n",
    "Wenn es Probleme gibt, bitte noch mal die obere Zelle laufen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62039c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Gradientenverfahren mit einer Lernrate von gamma = 2:\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=2, Iterationen=i, color = \"#00CC96\")\n",
    "contour_plot.show()\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f4425-5044-4b9e-81f2-70385c1a0eca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## Einfluss der Startwerts\n",
    "\n",
    "Wenn die zu minimierende Funktion mehrere lokale Minima besitzt, spielt die Wahl des Startwertes auch eine gro√üe Rolle: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff7250-8aed-4c9b-a5ee-1447587bbd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,6,-3,3,f)\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,6,-3,3,f)\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5647d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random.uniform\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[random.uniform(-5,6),random.uniform(-3,3)], function = f, grad=grad_f, gamma=1, Iterationen=30)\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39148f36-835e-4ef2-9073-5764c20f03f1",
   "metadata": {},
   "source": [
    "## Nichtlineare Optimierung mit Scipy\n",
    "\n",
    "Das vorgestellte Verfahren ist f√ºr die Praxis noch zu einfach, in der praktischen Anwendung gibt es noch viele Probleme, die zu beachten sind.\n",
    "Deswegen nimmt man normalerweise eine bereits vorhandene Implementierung.\n",
    "Hierf√ºr eignet sich das Python-Paket Scipy mit der Funktion `minimize` sehr gut.\n",
    "Es reicht, die zu minimierende Funktion und den Startwert einzugeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739a455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimierung mit scipy\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "minimize(fun=f, x0=np.array([-4,-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01f650-5cc6-4e3f-aca4-f3e8f6f5babd",
   "metadata": {},
   "source": [
    "## Nebenbedingungen der Form $g(x) \\leq 0$\n",
    "\n",
    "Ein Beispiel f√ºr eine Nebenbedingung der Form $g(x) \\leq 0$ ist\n",
    "\\begin{equation}\n",
    "-(x+2)^{2} + y^{3} \\leq 0\n",
    "\\end{equation}\n",
    "\n",
    "Dies definiert einen Bereich, in dem sich die L√∂sung befinden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160642d-9121-4f7c-86dc-9ebc109f8efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return (x[0]+3)**3-x[1]\n",
    "\n",
    "def g_(x):\n",
    "    return - ((x+2)**2)**(1/3)\n",
    "\n",
    "x1 = np.linspace(-5, -2, 100).tolist()\n",
    "y1 = [np.sqrt(np.abs(2.25 - (x+3.5)**2)) for x in x1]\n",
    "x2 = np.linspace(-2, -5, 100).tolist()\n",
    "y2 = [-np.sqrt(np.abs(2.25 - (x+3.5)**2)) for x in x2]\n",
    "x = x1 + x2\n",
    "y = y1 + y2\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "\n",
    "contour_plot.add_trace(go.Scatter(x=x, y=y, fill='tonexty', showlegend = False, marker = {'color' : '#19D3F3'})).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e55d28-a0cf-427a-a31e-b2c98f46150c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nebenbedingungen der Form $h(x) = 0$\n",
    "\n",
    "Eine Nebenbedingung der Form $h(x) = 0$ definiert √ºblicherweise eine Linie, auf der sich die L√∂sung befindet.\n",
    "Dies ist im Allgemeinen ein einschr√§nkende Bedingung, d.h. es wird ein schlechteres Ergebnis als ohne Nebenbedingung erreicht.\n",
    "Wir zeigen als Beispiel eine Nebenbedingung der Form\n",
    "\\begin{equation} h(x, y) = (x + 3)^2 - y. \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f321586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contour_plot.add_h()\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6e338-7adf-4cd4-b108-6e996c3d84bc",
   "metadata": {},
   "source": [
    "## Das quadratische Penalty-Verfahren\n",
    "\n",
    "Wir wissen jetzt, wie man unrestringierte nichtlineare Optimierungsprobleme l√∂sen kann.\n",
    "Doch wie k√∂nnen wir die Nebenbedingungen ber√ºcksichtigen, also\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{Minimiere }& f(x) \\\\\n",
    "\\textbf{wobei }& \\mathbf{g(x) \\leq 0} \\\\\n",
    "\\textbf{und }& \\mathbf{h(x) = 0} \\textbf{ gilt?}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Eine M√∂glichkeit ist es, zu der zu minimierenden Funktion $f$ einen Strafterm hinzuzuaddieren, der Abweichungen von den Nebenbedingungen bestraft.\n",
    "Solche Verfahren hei√üen **Penalty-Verfahren**.\n",
    "Wir stellen hier das quadratische **Penalty-Verfahren** vor.\n",
    "Das Problem l√§sst sich dann als\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Minimiere } f(x) + \\alpha \\max(0, g(x))^{2} + \\alpha h(x)^{2}, ~ \\alpha > 0\n",
    "\\end{equation}\n",
    "\n",
    "formulieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241e9da-308c-4e10-9e7b-02727bead3f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Das Penalty-Verfahren f√ºr $h(x) = 0$:\n",
    "\n",
    "Zur Erinnerung: \n",
    "\n",
    "\\begin{equation}h(x, y) = (x + 3)^2 - y.\\end{equation}\n",
    "\n",
    "Wir w√§hlen ein $\\alpha > 0$ und m√ºssen dann die Funktion\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f_{\\text{pen}} (x, y)\n",
    "&=\n",
    "f(x, y) + \\alpha h(x, y)^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "minimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394b1f5-1c79-42f8-8338-7f6d3f203451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "def h(x):\n",
    "    return (x[0]+3)**2 - x[1]\n",
    "\n",
    "def f_pen(x):\n",
    "    return f(x) + alpha*h(x)**2\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,f,opacity = 0.8, colorscale = 'blues')\n",
    "surface_plot.plot_surface(-5,1,-3,3,f_pen, opacity = 1, showscale=False, colorscale = 'oranges')\n",
    "surface_plot.show()\n",
    "surface_plot.write_html(\"bla.html\")\n",
    "\n",
    "def grad_h_sq(x):\n",
    "    return np.array([4*x[0]**3 + 36*x[0]**2 + 108*x[0] + 108 - 4*x[0]*x[1] - 12*x[1],\n",
    "                     -2*x[0]**2 - 12*x[0] + 2*x[1] - 18])\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "contour_plot.add_h()\n",
    "\n",
    "contour_plot.result = [0, 3]\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "x_range = 1\n",
    "y_range = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf10edc",
   "metadata": {},
   "source": [
    "Der Gradient von $h(x, y)^2$ ist\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla h^2(x,y) =\n",
    "\\begin{pmatrix}\n",
    "4x^{3} + 36x^{2} + 108x + 108 - 4xy - 12y \\\\\n",
    "-2x^{2} -12x + 2y - 18\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Der Gradient von $f_{ \\text{pen}}$ ist\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f_{ \\text{pen}} = \n",
    "\\nabla f + \\alpha \\nabla h.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c79ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot.add_gradient_descent(x0=contour_plot.result, function=f, grad=lambda x : (grad_f(x) + alpha*grad_h_sq(x)), gamma=gamma,\n",
    "                                  Iterationen=100, Nebenbedingung = h)\n",
    "\n",
    "contour_plot.show()\n",
    "\n",
    "x_result = contour_plot.result[0]\n",
    "y_result = contour_plot.result[1]\n",
    "\n",
    "contour_plot.contour_zoom(x_result-x_range, x_result+x_range, y_result-y_range, y_result+y_range, f)\n",
    "\n",
    "alpha *= 10\n",
    "gamma /= 10\n",
    "\n",
    "x_range /= 1.1\n",
    "y_range /= 1.1\n",
    "\n",
    "x0 = contour_plot.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6aca29",
   "metadata": {},
   "source": [
    "## Optimierung mit Nebenbedingungen in Python\n",
    "\n",
    "Auch mit Nebenbedingungen kann man in Python gut optimieren.\n",
    "\n",
    "### Optimierung mit $h(x, y) = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1194a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimierung mit scipy\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "def h(x):\n",
    "    return (x[0]+3)**2 - x[1]\n",
    "\n",
    "constraints = NonlinearConstraint(h, lb = 0, ub = 0)\n",
    "\n",
    "minimize(fun=f, x0=np.array([3,0]), constraints=constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e98de",
   "metadata": {},
   "source": [
    "### Optimierung mit $h(x, y) = 0$ und $g(x, y) \\leq 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return -(x[0]+2)**2 + x[1]**3\n",
    "\n",
    "constraints = [NonlinearConstraint(h, lb = 0, ub = 0),\n",
    "               NonlinearConstraint(g, lb = -np.inf, ub = 0)]\n",
    "\n",
    "minimize(fun=f, x0=np.array([-4,-2]), constraints=constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed26dfe",
   "metadata": {},
   "source": [
    "## Ausblick: Automatic Differentiation\n",
    "\n",
    "### Numerische Berechnung bei nicht bekannten Gradienten:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + h, y) - f(x, y)}{h}, ~ \n",
    "\\frac{\\partial f}{\\partial y} \\approx \\frac{f(x, y + h) - f(x, y)}{h}, ~\n",
    "h\\text{ klein}\n",
    "\\end{equation}\n",
    " \n",
    "$\\Rightarrow$ F√ºr jede partielle Ableitung muss ein eigener Differenzenquotient gebildet werden.\n",
    "\n",
    "- Besonders anspruchsvoller Anwendungsfall nichtlinearer Optimierung: Neuronale Netze\n",
    "- Gradientenverfahren wesentlicher Algorithmus zum Trainieren von neuronalen Netzen\n",
    "\n",
    "### Anwendung bei neuronalen Netzen\n",
    "\n",
    "![](simple_fnn_backprop.svg)\n",
    "<figcaption align = \"center\"><b>Graphik zeigt ein neuronales Netz zusammen mit der Loss-Funktion. Quelle: Maren Hackenberg, AG Machine Learning, Institut f√ºr Medizinische Biometrie\n",
    "und Statistik (IMBI), Universit√§t Freiburg, Quelle: </b> <a href=\"https://github.com/maren-ha/Masterarbeit/blob/master/Figures/simple_fnn_backprop.pdf\">github.com/maren-ha/Masterarbeit/blob/master/Figures/simple_fnn_backprop.pdf</a></figcaption>\n",
    "\n",
    "- Loss-Funktion $\\mathcal{L}$ h√§ngt von sehr vielen Parametern ab (z.B. $10\\,000 - 10\\,000\\,000$)\n",
    "- numerische Berechnung des Gradienten aufwendig und rundungsfehleranf√§llig\n",
    "- analytische Berechnung nicht praktikabel und abh√§ngig von der genauen Struktur des Netzes\n",
    "\n",
    "### L√∂sung: Automatic Differentiation\n",
    "\n",
    "- Automatic differentiation-Pakete k√∂nnen von (vielen) in Python definierten Funktionen die exakte Ableitung berechnen\n",
    "- Dadurch effizientere Optimierung hochdimensionaler Probleme m√∂glich \n",
    "- dieses Optimierungsverfahren, Gradientenverfahren + automatic differentiation, ist zentrales Element f√ºr den Erfolg von vielen Machine Learning Methoden\n",
    "\n",
    "### Beispiel: Effizienzsteigerung durch Automatic Differentiation\n",
    "\n",
    "Wir minimieren die Funktion:\n",
    "\\begin{equation}\n",
    "f(x) = \\exp \\left(- \\sum_{i=1}^{n} i \\cdot x_{i} \\right) + \\sum_{i=1}^{n} x_{i}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "\n",
    "def f(x):\n",
    "    return np.exp(-np.sum(np.array(range(1, len(x)+1) * x))) + np.sum(x**2)\n",
    "\n",
    "x0=np.ones(2)\n",
    "\n",
    "## Ohne Gradient\n",
    "minimize(fun=f, x0=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient berechnen\n",
    "grad_f = grad(f)\n",
    "minimize(fun=f, x0=x0, jac = grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarks\n",
    "\n",
    "x0=np.ones(100)\n",
    "\n",
    "t0 = time()\n",
    "for i in range(10):\n",
    "    fd = minimize(fun=f, x0=x0)\n",
    "t_fd = (time() - t0) / 10\n",
    "\n",
    "print(\"Funktionswert Finite Difference: \", fd.fun)\n",
    "print(\"Rechenzeit Finite Difference: \", t_fd)\n",
    "\n",
    "#hess_f = hessian(f)\n",
    "t0 = time()\n",
    "for i in range(10):\n",
    "    grad_f = grad(f)\n",
    "    autodiff = minimize(fun=f, x0=x0, jac = grad_f)\n",
    "t_autodiff = (time() - t0) / 10\n",
    "\n",
    "print(\"Funktionswert Autodiff: \", autodiff.fun)\n",
    "print(\"Rechenzeit Autodiff: \", t_autodiff)\n",
    "print(\"(Rechenzeit Finite Difference) / (Rechenzeit Autodiff):\", t_fd / t_autodiff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
