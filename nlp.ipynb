{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366edae6-c36d-4b71-9d27-83de8f4d7ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Nichtlineare Optimierung mit Nebenbedingungen\n",
    "\n",
    "## Dr. Anselm Hudde - 24.01.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda742a-3bc3-4d75-9c5d-867099ff875c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run nonlinear_programming.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e78631",
   "metadata": {},
   "source": [
    "## Worum geht es bei der nichtlinearen Optimierung?\n",
    "\n",
    "Bei der Optimierung geht es darum, das Minimum (bzw. das Maximum) einer Funktion $f$ zu finden.\n",
    "\n",
    "- Wie maximiert ein Unternehmen den Ertrag, unter Ber√ºcksichtigung der vorhandenen Ressourcen?\n",
    "\n",
    "- Wie erreicht man mit einer Portfolioallokation eines maximale Rendite, wenn die erlaubte Volatilit√§t beschr√§nkt ist?\n",
    "\n",
    "- Wie kann man die richtigen Modellparameter sch√§tzen (statistische Modelle, neuronale Netze)?\n",
    "\n",
    "## Beispiel aus der Portfoliooptimierung:\n",
    "\n",
    "Ein Investor will $10\\,000$ Euro Investieren, wobei diese auf zwei Aktien, $A$ und $B$, aufgeteilt werden.\n",
    "\n",
    "- Erwartete j√§hrliche Renditen: \n",
    "\n",
    "    Aktie $A$: $r_A = 7\\%$\n",
    "\n",
    "    Aktie $B$: $r_{A} = 9\\%$\n",
    "    \n",
    "    \n",
    "- Volatilit√§ten:\n",
    "\n",
    "    Aktie $A$: $\\sigma_A = 20\\%$\n",
    "\n",
    "    Aktie $B$: $\\sigma_{B} = 30\\%$\n",
    "    \n",
    "    \n",
    "- Korrelation:\n",
    "\n",
    "    $\\rho_{A, B} = 0.7$\n",
    "\n",
    "Der Investor m√∂chte sein Geld so anlegen, dass er eine erwartete Rendite von $5\\%$ erreicht, und dabei so wenig Volatilit√§t wie m√∂glich zulassen.\n",
    "Die Volatilit√§t eines Portfolios bestehen aus $x_1$ Euro in Aktie 1 und $x_2$ Euro in in Aktie 2 berechnet sich als\n",
    "\n",
    "\\begin{equation}\n",
    "f (x_{1}, x_{2})\n",
    "=\n",
    "\\sqrt{ \\sigma_1^2 x_1^2 + \\sigma_2^2 x_2^2 + 2 \\sigma_1 x_1 \\sigma_2 x_2 \\rho_{1, 2}}\n",
    "\\end{equation}\n",
    "wobei $\\theta$ die Risikoaversion des Investor darstellt.\n",
    "\n",
    "Unser Ziel ist also: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{Minimiere }& f(x) \\\\\n",
    "\\text{wobei }& x_1 + x_2 - 1 \\leq 0 \\\\\n",
    "\\text{und }& \\frac{r_1 * x_{1} + r_{2} * x_{2}}{10\\,000} - 0.05 = 0 \\text{ gilt.}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Wenn wir\n",
    "\n",
    "\\begin{equation}\n",
    "g(x_1, x_2) = x_1 + x_2 - 1.\n",
    "\\end{equation}\n",
    "\t\n",
    "\\begin{equation}\n",
    "h(x_1, x_2) = \\frac{r_1 * x_{1} + r_{2} * x_{2}}{10\\,000}\n",
    "\\end{equation}\n",
    "\n",
    "definieren haben wir auch eine allgemeine Defintion des Problems:\n",
    "\n",
    "## Formulierung eines nichtlinearen Optimierungsproblems mit Nebenbedingungen\n",
    "\n",
    "Sei $D \\subseteq \\mathbb{R}^n$ eine Teilmenge, und seien\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f \\colon D \\to \\mathbb{R} \\\\\n",
    "g \\colon D \\to \\mathbb{R} \\\\\n",
    "h \\colon D \\to \\mathbb{R}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "drei (m√∂glicherweise nichtlineare) Funktionen.\n",
    "Das Ziel der nichtlinearen Optimierung:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{Minimiere }& f(x) \\\\\n",
    "\\text{wobei }& g(x) \\leq 0 \\\\\n",
    "\\text{und }& h(x) = 0 \\text{ gilt.}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb3a6e-f567-4a67-8a00-e22646a019bb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## Beispielfunktion\n",
    "\n",
    "Wir betrachten die Funktion\n",
    "\n",
    "\\begin{equation}f(x, y) = sin(x) + 0.05 \\cdot x^2 + 0.1 \\cdot y^2, \\end{equation}\n",
    "\n",
    "die wir minimieren wollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85e606",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,f)\n",
    "surface_plot.show()\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406e17b-70d2-4f3f-81b2-3354f42acf0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Das Gradientenverfahren: Idee\n",
    "\n",
    "Die Idee ist es, immer in die Richtung zu gehen, in die es am steilsten bergab geht.\n",
    "Doch wie messen wir die Steigung?\n",
    "Hier kommt der **Gradient** ins Spiel:\n",
    "\n",
    "**Definition:**\n",
    "Der Gradient der Funktion $f \\colon \\mathbb{R}^{n} \\to \\mathbb{R}$ ist die Funktion\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f \\colon \\mathbb{R}^{n} \\to \\mathbb{R}^{n};~~\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_{1} \\\\ \\vdots \\\\ x_{n}\n",
    "\\end{pmatrix}\n",
    "\\mapsto\n",
    "\\begin{pmatrix}\n",
    "\\tfrac{\\partial f}{x_{1}} (x) \\\\ \\vdots \\\\ \\tfrac{\\partial f}{x_{n}} (x)\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf3abe-4911-45c0-99b7-78eb125d5cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Anschauung:** Der Gradient zeigt immer in Richtung des st√§rksten Anstiegs.\n",
    "\n",
    "Beispiel 1: $f(x, y) = x$, $\\nabla f(x, y) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392913a-0e0c-4456-8938-893f47d3cd1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,function = lambda x: x[0])\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13632e34-1665-4598-8873-2bc804728365",
   "metadata": {},
   "source": [
    "Beispiel 1: $f(x, y) = \\tfrac{1}{3} (x+y)$, $\\nabla f(x, y) = \\begin{pmatrix} \\tfrac{1}{3} \\\\ \\tfrac{1}{3} \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cac26d-b0e0-45c8-aa7e-34766f499044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,function = lambda x: (1/3)*(x[0]+x[1]))\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635e17b-370a-41e8-9254-f9d3139664f6",
   "metadata": {},
   "source": [
    "## Zur√ºck zur eigentlichen Funktion:\n",
    "\n",
    "Der Gradient von\n",
    "$ùëì(ùë•,ùë¶)= ùë†ùëñùëõ(ùë•)+0.05\\cdot ùë•^2 + 0.1 \\cdot ùë¶^2$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(x, y)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\cos(x) + 0.1x \\\\ 0.2 y\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Beispiele: \n",
    "\\begin{equation}\n",
    "\\nabla f(-4, -2) =\n",
    "\\begin{pmatrix}\n",
    "-1.05 \\\\ -0.40\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\nabla f(-1, 2) =\n",
    "\\begin{pmatrix}\n",
    "0.44 \\\\ 0.20\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db4c94-24fe-4804-90df-351454a787df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return np.array([np.cos(x[0])+0.1*x[0], 0.2*x[1]])\n",
    "\n",
    "contour_plot.add_gradients(grad_f)\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5117f6f",
   "metadata": {},
   "source": [
    "## Die Schritte beim Gradientenverfahren\n",
    "\n",
    "Wir starten mit einem Punkt $x_0 \\in \\mathbb{R}^{2}$.\n",
    "Dann gehen wir mit jeden Schritt in die Gegenrichtung des Gradienten\n",
    "$\\nabla f(x, y)$, und zwar multipliziert mit der Lernrate $\\gamma_n$:\n",
    "\n",
    "\\begin{equation}\n",
    "\tx_{n+1}= x_{n} - \\gamma_{n} \\nabla f({x} _{n}),\\ n\\geq 0.\n",
    "\\end{equation}\n",
    "Wenn die Lernrate dann klein genug ist, gilt\n",
    "$f(x_{n+1}) \\leq f(x_n)$, und das Verfahren konvergiert.\n",
    "\n",
    "### Lernrate $\\gamma = 1$:\n",
    "\n",
    "Wir fangen im Punkt\n",
    "$x_0 = \\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}$\n",
    "an, und verwenden eine Lernrate von $\\gamma = 1$.\n",
    "Dann gilt\n",
    "\n",
    "\\begin{equation} \\nabla f(x_0) =\n",
    "\\begin{pmatrix}\\cos(-4)+0.1\\cdot(-4) \\\\ 0.2\\cdot(-2) \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} -1.05 \\\\ -0.40 \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "und damit\n",
    "\n",
    "\\begin{equation}\n",
    "x_{1} = x_{0} + \\gamma_{0} \\cdot \\nabla f(x_0)\n",
    "=\n",
    "\\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}\n",
    "- 1 \\cdot \n",
    "\\begin{pmatrix} -1.05 \\\\ -0.40 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "-2.95 \\\\ -1.60\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "Die n√§chsten Schritte folgen analog:\n",
    "\n",
    "$x_{2} = \\begin{pmatrix} -1.67 \\\\ -1.28\\end{pmatrix}, ~\n",
    "x_{3} = \\begin{pmatrix} -1.40 \\\\ -1.28\\end{pmatrix}$\n",
    "\n",
    "usw..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea11cba-4570-4aaf-b9cf-6e1f4b898999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Das Gradientenverfahren mit einer Lernrate von gamma = 1:\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return np.array([np.cos(x[0])+0.1*x[0], 0.2*x[1]])\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5617af",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=1, Iterationen=i, color = \"#636EFA\")\n",
    "contour_plot.show() # noch 3d-plot??\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d308b5e-5660-44a2-aabe-cf2e385b9d8a",
   "metadata": {},
   "source": [
    "### Das Gradientenverfahren mit einer Lernrate von $\\gamma = 0.1$ ist langsamer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e363b-6d69-4bd9-84a6-9ca6a5d5fed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669954b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Gradientenverfahren mit einer Lernrate von gamma = 0.1:\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=0.1, Iterationen=i, color = \"#EF553B\")\n",
    "\n",
    "contour_plot.show()\n",
    "\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba80109",
   "metadata": {},
   "source": [
    "### Eine Lernrate von $\\gamma = 2$ f√ºhrt hingegen nicht zu einer Konvergenz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd4ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62039c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Gradientenverfahren mit einer Lernrate von gamma = 2:\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=2, Iterationen=i, color = \"#00CC96\")\n",
    "\n",
    "contour_plot.show() # noch 3d-plot??\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f4425-5044-4b9e-81f2-70385c1a0eca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## Einfluss der Startwerts\n",
    "\n",
    "Wenn die zu minimierende Funktion mehrere globale Minima besitzt, spielt die Wahl des Startwertes auch eine gro√üe Rolle: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff7250-8aed-4c9b-a5ee-1447587bbd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,6,-3,3,f)\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,6,-3,3,f)\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5647d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[random.uniform(-5,6),random.uniform(-3,3)], function = f, grad=grad_f, gamma=1, Iterationen=10)\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39148f36-835e-4ef2-9073-5764c20f03f1",
   "metadata": {},
   "source": [
    "## Nichtlineare Optimierung mit Scipy\n",
    "\n",
    "Das vorgestellte Verfahren ist f√ºr die Praxis noch zu einfach, in der praktischen Anwendung gibt es noch viele Probleme, die zu beachten sind.\n",
    "Deswegen nimmt man normalerweise eine bereits vorhandene Implementierung.\n",
    "Hierf√ºr eignet sich das Python-Paket Scipy mit der Funktion *minimize* sehr gut.\n",
    "Es reicht, die zu minimierende Funktion und den Startwert einzugeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739a455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimierung mit scipy\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "minimize(fun=f, x0=np.array([-4,-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e61ae",
   "metadata": {},
   "source": [
    "### Bessere Performance druch Eingabe des Gradienten\n",
    "\n",
    "Wenn man zus√§tzlichn den Gradienten als Argument √ºbergibt, konvergiert das Verfahren schneller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a709b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return np.array([np.cos(x[0])+0.1*x[0], 0.2*x[1]])\n",
    "\n",
    "minimize(fun=f, x0=np.array([-4,-2]), jac=grad_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01f650-5cc6-4e3f-aca4-f3e8f6f5babd",
   "metadata": {},
   "source": [
    "## Nebenbedingungen der Form $g(x) \\leq 0$\n",
    "\n",
    "Ein Beispiel f√ºr eine Nebenbedingung der Form $g(x) \\leq 0$ ist\n",
    "\\begin{equation}\n",
    "-(x+2)^{2} + y^{3} \\leq 0\n",
    "\\end{equation}\n",
    "\n",
    "Dies Definiert einen Bereich, in dem sich die L√∂sung befinden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160642d-9121-4f7c-86dc-9ebc109f8efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return (x[0]+3)**3-x[1]\n",
    "\n",
    "def g_(x):\n",
    "    return - ((x+2)**2)**(1/3)\n",
    "\n",
    "x1 = np.linspace(-5, -2, 100).tolist()\n",
    "y1 = [np.sqrt(np.abs(2.25 - (x+3.5)**2)) for x in x1]\n",
    "x2 = np.linspace(-2, -5, 100).tolist()\n",
    "y2 = [-np.sqrt(np.abs(2.25 - (x+3.5)**2)) for x in x2]\n",
    "x = x1 + x2\n",
    "y = y1 + y2\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "\n",
    "contour_plot.add_trace(go.Scatter(x=x, y=y, fill='tonexty', showlegend = False, marker = {'color' : 'cyan'})).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e55d28-a0cf-427a-a31e-b2c98f46150c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nebenbedingungen der Form $h(x) = 0$\n",
    "\n",
    "Eine Nebenbedingung der Form $h(x) = 0$ definiert √ºberlicherweise eine Linie, auf der sich L√∂sung befindet.\n",
    "Dies ist im allgemeinen ein aktive Bedingung, d.h. es wird ein schlechteres Ergebnis als ohne Nebenbedingung erreicht.\n",
    "Wir zeigen als Beispiel eine Nebenbedingung der Form\n",
    "\\begin{equation} h(x, y) = (x + 3)^2 - y. \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f321586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contour_plot.add_h()\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6e338-7adf-4cd4-b108-6e996c3d84bc",
   "metadata": {},
   "source": [
    "## Das quadratische Penalty-Verfahren\n",
    "Wir wissen jetzt, wie man unrestringierte nichtlineare Optimierungsprobleme l√∂sen kann.\n",
    "Doch wie k√∂nnen wir die Nebenbedingungen ber√ºcksichtigen, also\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{Minimiere }& f(x) \\\\\n",
    "\\text{wobei }& g(x) \\leq 0 \\\\\n",
    "\\text{und }& h(x) = 0 \\text{ gilt?}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Eine M√∂glichkeit ist es, zu der zu minimierenden Funktion $f$ einen Strafterm hinzuzuaddieren, der Abweichungen von den Nebenbedingungen bestraft.\n",
    "Solche Verfahren hei√üen **Penalty-Verfahren**.\n",
    "Wir stellen hier das quadratische **Penalty-Verfahren** vor.\n",
    "Das Problem l√§sst sich dann als\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Minimiere } f(x) + \\tfrac{ \\alpha}{2} max(0, g(x))^{2} + \\tfrac{ \\alpha}{2} h(x)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "formulieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241e9da-308c-4e10-9e7b-02727bead3f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Das Penalty-Verfahren f√ºr $h(x) = 0$:\n",
    "\n",
    "Zur Erinnerung: \n",
    "\n",
    "\\begin{equation}h(x, y) = (x + 3)^2 - y.\\end{equation}\n",
    "\n",
    "Wir w√§hlen ein $\\alpha \\in \\mathbb{R}$ und m√ºssen dann die Funktion\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f_{\\text{pen}} (x, y)\n",
    "&=\n",
    "f(x, y) + \\alpha h(x, y)^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "minimieren.\n",
    "Der Gradient von $h(x, y)^2$ ist\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla h^2(x,y) =\n",
    "\\begin{pmatrix}\n",
    "4x^{3} + 36x^{2} + 108x + 108 - 4xy - 12y \\\\\n",
    "-2x^{2} -12x + 2y - 18\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Der Gradient von $f_{ \\text{pen}}$ ist\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f_{ \\text{pen}} = \n",
    "\\nabla f + \\alpha \\nabla g.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394b1f5-1c79-42f8-8338-7f6d3f203451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "def h(x):\n",
    "    return (x[0]+3)**2 - x[1]\n",
    "\n",
    "def f_pen(x):\n",
    "    return f(x) + alpha*h(x)**2\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,f,opacity = 0.2)\n",
    "surface_plot.plot_surface(-5,1,-3,3,f_pen, opacity = 0.8, showscale=False, colorscale = 'oxy')\n",
    "surface_plot.show()\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "contour_plot.add_h()\n",
    "\n",
    "def grad_h_sq(x):\n",
    "    return np.array([4*x[0]**3 + 36*x[0]**2 + 108*x[0] + 108 - 4*x[0]*x[1] - 12*x[1],\n",
    "                     -2*x[0]**2 - 12*x[0] + 2*x[1] - 18])\n",
    "\n",
    "x0=[0, 3]\n",
    "\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot.add_gradient_descent(x0=x0, function=f, grad=lambda x : (grad_f(x) + alpha*grad_h_sq(x)), gamma=gamma,\n",
    "                                  Iterationen=100, Nebenbedingung = h)\n",
    "\n",
    "contour_plot.show()\n",
    "\n",
    "alpha *= 1.5\n",
    "gamma /= 1.5\n",
    "x0 = contour_plot.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6aca29",
   "metadata": {},
   "source": [
    "## Optimierung mit Nebenbedingungen in Python\n",
    "\n",
    "Auch mit Nebenbedingungen kann man in Python gut optimieren.\n",
    "\n",
    "### Optimierung mit $h(x, y) = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1194a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimierung mit scipy\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "def h(x):\n",
    "    return (x[0]+3)**2 - x[1]\n",
    "\n",
    "constraints =NonlinearConstraint(h, lb = 0, ub = 0)\n",
    "\n",
    "minimize(fun=f, x0=np.array([3,0]), constraints=constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e98de",
   "metadata": {},
   "source": [
    "### Optimierung mit $h(x, y) = 0$ und $g(x, y) \\leq 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return -(x[0]+2)**2 + x[1]**3\n",
    "\n",
    "constraints = [NonlinearConstraint(h, lb = 0, ub = 0),\n",
    "               NonlinearConstraint(g, lb = -np.inf, ub = 0)]\n",
    "\n",
    "minimize(fun=f, x0=np.array([-4,-2]), constraints=constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed26dfe",
   "metadata": {},
   "source": [
    "## Ausblick: Automatic Differentiation\n",
    "\n",
    "Optimierungsprobleme lassen sich oft besser l√∂sen, wenn der Gradient der zu minimierenden Funktion bekannt ist.\n",
    "In der Praxis sind die Ableitungen aber zu schwierig zu berechnen.\n",
    "**Automatic differentation** schafft hier Abhilfe:\n",
    "Entsprechende Pakete k√∂nnen von vielen in Python definierten Funktionen die mathematische Ableitung bestimmen (im Gegensatz zur numerischen Ableitung).\n",
    "Dies erm√∂glicht auch eine effiziente Implementierung der Optimierung bei komplizierten Zielfunktionen, und eine bessere Anwendbarkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np   # Thinly-wrapped version of Numpy\n",
    "from autograd import grad, hessian\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "    \n",
    "grad_f = grad(f)\n",
    "\n",
    "print(minimize(fun=f, method = 'SLSQP', x0=np.array([-4,-2])))\n",
    "\n",
    "hess_f = hessian(f)\n",
    "\n",
    "minimize(fun=f, jac = grad_f, hess = hess_f, method= 'Newton-CG', x0=np.array([-4,-2]))\n",
    "\n",
    "# nit ist die Anzahl der Iterationen\n",
    "# nfev : number of function evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a735f0b1",
   "metadata": {},
   "source": [
    "### Automatic differentiation verbessert auch das Konvergenzverhalten f√ºr die Optimierung mit Nebenbedingungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d06c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimierung mit scipy\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "def h(x):\n",
    "    return (x[0]+3)**2 - x[1]\n",
    "def g(x):\n",
    "    return (-(x[0]+2)**2 + x[1]**3)\n",
    "\n",
    "constraints = [NonlinearConstraint(h, lb = 0, ub = 0),\n",
    "               NonlinearConstraint(g, lb = -np.inf, ub = 0)]\n",
    "    \n",
    "minimize(fun=f, x0=np.array([-4,-2]), constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fd9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_h = grad(h)\n",
    "\n",
    "def hess_h(x, y):\n",
    "    np.matmul(y, hessian(h)(x))\n",
    "grad_g = grad(g)\n",
    "#hess_h = hessian(h)\n",
    "grad_f = grad(f)\n",
    "hess_f = hessian(f)\n",
    "\n",
    "#def hess_h(m):\n",
    "#    return np.matmul(hess_h_, m)\n",
    "\n",
    "def hess_g(m):\n",
    "    return np.matmul(hess_h_, m)\n",
    "\n",
    "constraints = [NonlinearConstraint(h, lb = 0, ub = 0, jac = grad_h),\n",
    "               NonlinearConstraint(g, lb = -np.inf, ub = 0, jac = grad_g)]\n",
    "    \n",
    "minimize(fun=f, x0=np.array([-4,-2]), jac = grad_f, hess = hess_f, constraints=constraints)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
